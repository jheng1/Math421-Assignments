{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t4119 obs. of  16 variables:\n",
      " $ age        : num  30 39 25 38 47 32 32 41 31 35 ...\n",
      " $ job        : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 2 8 8 8 1 8 1 3 8 2 ...\n",
      " $ marital    : Factor w/ 4 levels \"divorced\",\"married\",..: 2 3 2 2 2 3 3 2 1 2 ...\n",
      " $ education  : Factor w/ 8 levels \"basic.4y\",\"basic.6y\",..: 3 4 4 3 7 7 7 7 6 3 ...\n",
      " $ default    : Factor w/ 3 levels \"no\",\"unknown\",..: 1 1 1 1 1 1 1 2 1 2 ...\n",
      " $ housing    : Factor w/ 3 levels \"no\",\"unknown\",..: 3 1 3 2 3 1 3 3 1 1 ...\n",
      " $ loan       : Factor w/ 3 levels \"no\",\"unknown\",..: 1 1 1 2 1 1 1 1 1 1 ...\n",
      " $ contact    : Factor w/ 2 levels \"cellular\",\"telephone\": 1 2 2 2 1 1 1 1 1 2 ...\n",
      " $ month      : Factor w/ 10 levels \"apr\",\"aug\",\"dec\",..: 7 7 5 5 8 10 10 8 8 7 ...\n",
      " $ day_of_week: Factor w/ 5 levels \"fri\",\"mon\",\"thu\",..: 1 1 5 1 2 3 2 2 4 3 ...\n",
      " $ duration   : num  487 346 227 17 58 128 290 44 68 170 ...\n",
      " $ campaign   : num  2 4 1 3 1 3 4 2 1 1 ...\n",
      " $ previous   : num  0 0 0 0 0 2 0 0 1 0 ...\n",
      " $ poutcome   : Factor w/ 3 levels \"failure\",\"nonexistent\",..: 2 2 2 2 2 1 2 2 1 2 ...\n",
      " $ nr.employed: num  5099 5191 5228 5228 5196 ...\n",
      " $ y          : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>no</dt>\n",
       "\t\t<dd>3668</dd>\n",
       "\t<dt>yes</dt>\n",
       "\t\t<dd>451</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[no] 3668\n",
       "\\item[yes] 451\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "no\n",
       ":   3668yes\n",
       ":   451\n",
       "\n"
      ],
      "text/plain": [
       "  no  yes \n",
       "3668  451 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " no yes \n",
       "  0 316 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "  no  yes \n",
       "2568    0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Balanced Accuracy:</strong> 0.880420875420875"
      ],
      "text/latex": [
       "\\textbf{Balanced Accuracy:} 0.880420875420875"
      ],
      "text/markdown": [
       "**Balanced Accuracy:** 0.880420875420875"
      ],
      "text/plain": [
       "Balanced Accuracy \n",
       "        0.8804209 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"Width not defined. Set with `position_dodge(width = ?)`\""
     ]
    }
   ],
   "source": [
    "# For this project, we are analyzing a dataset using R. The dataset I will be analyzing is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls to clients. I got this dataset through UCI Machine Learning Repository to find a dataset that meets the requirements of the project. The goal of this dataset is to predict if the client will subscribe a term deposit (variable y). A term deposit is a cash investment held at a financial institution over a period of time. This is relevant because it predicts how effective some methods are for increasing the possibility for subscribing to a term deposit. \n",
    "  \n",
    " # I used a portion of this dataset with 4120 observations. There are a total of 21 variables\n",
    "#including age, job, marital status, education, if the client has credit in default, has a housing loan, has a personal loan, type of contact, last contact month of year, day of week, duration of phone call, campaign, number of days passed from previous campaign, number of contacts from previous campaign, outcome of previous marketing campaign, employment variation rate, consumer price index, consumer confidence index, number of employees, euribor 3 month rate, and the target variable (y) if the client subscribed a term deposit. The target variable is binary. The dataset consisted of 9 numeric variables, and the rest were categorical variables.\n",
    "\n",
    "\n",
    "library(\"readxl\")\n",
    "bank= read_excel(\"C:/Users/student/Downloads/R/bank_rev.xlsx\")\n",
    "bank$pdays=NULL\n",
    "bank$emp.var.rate=NULL\n",
    "bank$euribor3m=NULL\n",
    "bank$cons.conf.idx=NULL\n",
    "bank$cons.price.idx=NULL\n",
    "bank$y=factor(bank$y)\n",
    "bank$job=factor(bank$job)\n",
    "bank$marital=factor(bank$marital)\n",
    "bank$education=factor(bank$education)\n",
    "bank$default=factor(bank$default)\n",
    "bank$housing=factor(bank$housing)\n",
    "bank$loan=factor(bank$loan)\n",
    "bank$contact=factor(bank$contact)\n",
    "bank$month=factor(bank$month)\n",
    "bank$day_of_week=factor(bank$day_of_week)\n",
    "bank$poutcome=factor(bank$poutcome)\n",
    "str(bank)\n",
    "\n",
    "\n",
    "#check imbalanced data\n",
    "library(caret)\n",
    "summary(bank$y)\n",
    "\n",
    "\n",
    "#I noticed that my target variable was imbalanced because there were a lot more people that did not subscribe to a term deposit than people who agreed to subscribe to a term deposit. I addressed this issue by implementing undersampling to balance the data. After performing undersampling, I got a balanced accuracy of roughly 87%.\n",
    "\n",
    "#undersampling\n",
    "library(ranger)\n",
    "library(caret)\n",
    "splitIndex=createDataPartition(bank$y, p=.70, list=FALSE, times=1)\n",
    "train_bank=bank[splitIndex,]\n",
    "test_bank=bank[-splitIndex,]\n",
    "\n",
    "train1=train_bank[train_bank$y==\"yes\",]\n",
    "n1=nrow(train1)\n",
    "table(train1$y)\n",
    " \n",
    "train0=train_bank[train_bank$y==\"no\",]\n",
    "n0=nrow(train0)\n",
    "table(train0$y)\n",
    " \n",
    "train00=train0[sample(1:n0,n1, replace=TRUE),]\n",
    " \n",
    "train_under=rbind(train00,train1)\n",
    " \n",
    "model_under=ranger(y~.,data=train_under)\n",
    "pred_under=predict(model_under,data=test_bank)$predictions\n",
    "\n",
    "cm_under=confusionMatrix(pred_under,test_bank$y,positive=\"yes\")\n",
    "cm_under$byClass['Balanced Accuracy']\n",
    "\n",
    "#graphs\n",
    "library(ggplot2)\n",
    "ggplot(bank)+geom_bar(mapping = aes(x=y, fill=y), position =\"dodge\")+labs(title=\"Submitted term deposit\", x=\"Default\", y=\"Number of Default\")\n",
    "#This shows the number of people who did not submit a term deposit\n",
    "ggplot(bank)+geom_density(mapping = aes(x=age, color=y), position =\"dodge\")+labs(title=\"Default on Loan\", x=\"age\", y=\"Default\")\n",
    "#This shows that more people between the ages of 30-50 did not submit term deposit\n",
    "ggplot(bank)+geom_bar(mapping = aes(x=marital, fill=y), position =\"dodge\")+labs(title=\"marital status vs term deposit\", x=\"marital status\", y=\"Deposit\")\n",
    "#This shows that more married people did not submit a term deposit\n",
    "ggplot(bank)+geom_bar(mapping = aes(x=day_of_week, fill=y), position =\"dodge\")+labs(title=\"day of weekvs term deposit\", x=\"day of week\", y=\"Deposit\")\n",
    "#This shows that it is pretty even of how many people do or do not submit term deposit in a typical day\n",
    "ggplot(bank)+geom_bar(mapping = aes(x=month, fill=y), position =\"dodge\")+labs(title=\"month subscribed\", x=\"month\", y=\"Deposit\")\n",
    "#There are more people who do not submit a tern deposit in May\n",
    "ggplot(bank)+geom_bar(mapping = aes(x=loan, fill=y), position =\"dodge\")+labs(title=\"Default on Loan\", x=\"loan\", y=\"Deposit\")\n",
    "#This shows that most people who did not default on a loan did not sumbit a term deposit, but they would not need to. However there are around 550 people that defaulted on a loan and did not submit a term deposit\n",
    "ggplot(bank)+geom_bar(mapping = aes(x=housing, fill=y), position =\"dodge\")+labs(title=\"housing loan\", x=\"loan\", y=\"Deposit\")\n",
    "#This shows how many people who took out a housing loan did not submit a term deposit\n",
    "ggplot(bank)+geom_density(mapping = aes(x=duration, color=y), position =\"dodge\")+labs(title=\"duration\", x=\"duration (seconds)\", y=\"Deposit\")\n",
    "#The duration in seconds is the amount of time an employee got a hold of the customer over the phone, which shows that more people said yes if the phone call was longer.\n",
    "ggplot(bank)+geom_density(mapping = aes(x=campaign, fill=y), position =\"dodge\")+labs(title=\"Campaign for term deposit\", x=\"number of times contacted\", y=\"Deposit\")\n",
    "#This shows that the more times people were contacted, they said no to subscribing to a term deposit.\n",
    "ggplot(bank)+geom_density(mapping = aes(x=nr.employed, fill=y), position =\"dodge\")+labs(title=\"number of employees\", x=\"number of employees\", y=\"Deposit\")\n",
    "#This shows that when there were less employees working, more people said yes to a term deposit\n",
    "ggplot(bank)+geom_bar(mapping = aes(x=education, fill=y), position =\"dodge\")+labs(title=\"education\", x=\"education\", y=\"Deposit\")\n",
    "#There were more people who said no to a term deposit that had a university degree.\n",
    "ggplot(bank)+geom_density(mapping = aes(x=previous, color=y), position =\"dodge\")+labs(title=\"previous outcome\", x=\"loan\", y=\"Deposit\")\n",
    "#People were more likely to say yes to a term deposit if they were contacted earlier\n",
    "ggplot(bank)+geom_bar(mapping = aes(x=poutcome, fill=y), position =\"dodge\")+labs(title=\"previous outcome\", x=\"loan\", y=\"Deposit\")\n",
    "#There are more people who say yes to a term deposit if they were not contacted before\n",
    "\n",
    "#To impute missing values, I imputed numeric by mean and categorical by mode. I then ran four models: decision tree, random forest, glm, and glmnet to see which one provided a better model.\n",
    "\n",
    "#impute by mean\n",
    "misshandled=function(data){\n",
    "  for(i in 1:ncol(data)){\n",
    "    if (is.numeric(data[,i])){\n",
    "      data[,i][is.na(data[,i])]=mean(data[,i], na.rm=TRUE)\n",
    "    } else{\n",
    "      levels=unique(data[,i])\n",
    "      data[,i][is.na(data[,i])]=levels[which.max(tabulate(match(data[,i],levels)))]\n",
    "    }\n",
    "  }\n",
    "  return(data)\n",
    "}\n",
    "bank1=misshandled(bank)\n",
    "sum(is.na(bank1))\n",
    "\n",
    "#model\n",
    "set.seed(200)\n",
    "splitIndex<- createDataPartition(bank1$y, p=.7, list=FALSE, times = 1)\n",
    "train<- bank1[splitIndex,]\n",
    "test<- bank1[-splitIndex,]\n",
    "\n",
    "#decision tree model\n",
    "\n",
    "model <- train(y~.,data =train, method = \"rpart\")\n",
    "pred=predict(model,test)\n",
    "levels(test$y) = c(\"1\", \"0\")\n",
    "levels(pred) = c(\"1\", \"0\")\n",
    "cm=confusionMatrix(pred, test$y, positive=\"1\")\n",
    "cm\n",
    "\n",
    "  \n",
    "#random forest\n",
    "  \n",
    "modelrf = ranger(y ~., data = train)\n",
    "pred1  = predict(modelrf, data = test)$predictions\n",
    "levels(test$y) = c(\"0\", \"1\")\n",
    "levels(pred1) = c(\"0\", \"1\")\n",
    "cm1=confusionMatrix(pred1, test$y, positive=\"1\")\n",
    "cm1\n",
    "\n",
    "  \n",
    "#glm\n",
    "  library(glmnet)\n",
    "modelglm <- train(\n",
    "  y~.,\n",
    "  data = bank, method = \"glm\",\n",
    "  trControl = trainControl(method = \"cv\", number = 7, verboseIter = TRUE))\n",
    "pred2=predict(modelglm,test)\n",
    "levels(test$y) = c(\"0\", \"1\")\n",
    "levels(pred2) = c(\"0\", \"1\")\n",
    "cm2=confusionMatrix(pred2, test$y, positive=\"1\")\n",
    "cm2\n",
    "\n",
    "#glmnet\n",
    "\n",
    "modelglmnet <- train(\n",
    "  y~.,\n",
    "  data = bank, method = \"glmnet\",\n",
    "  trControl = trainControl(method = \"cv\", number = 7, verboseIter = TRUE))\n",
    "pred3=predict(modelglmnet,test)\n",
    "levels(test$y) = c(\"0\", \"1\")\n",
    "levels(pred3) = c(\"0\", \"1\")\n",
    "cm3=confusionMatrix(pred3, test$y, positive=\"1\")\n",
    "cm3\n",
    "\n",
    "bank[bank==\"unknown\"]=NA\n",
    "sum(is.na(bank))\n",
    "\n",
    "#Another way I tried to impute missing variables was by excluding the missing data. By doing so, I found that the models gave similar results, but generally imputing by mean and mode gave better models.\n",
    "\n",
    "#exclude missing data\n",
    "bank3=na.exclude(bank)\n",
    "sum(is.na(bank3))\n",
    "\n",
    "#model\n",
    "set.seed(200)\n",
    "splitIndex<- createDataPartition(bank3$y, p=.7, list=FALSE, times = 1)\n",
    "train<- bank3[splitIndex,]\n",
    "test<- bank3[-splitIndex,]\n",
    "\n",
    "#decision tree model\n",
    "\n",
    "model <- train(y~.,data =train, method = \"rpart\")\n",
    "pred=predict(model,test)\n",
    "levels(test$y) = c(\"1\", \"0\")\n",
    "levels(pred) = c(\"1\", \"0\")\n",
    "cm=confusionMatrix(pred, test$y, positive=\"1\")\n",
    "cm\n",
    "\n",
    "  \n",
    "#random forest\n",
    "  \n",
    "modelrf = ranger(y ~., data = train)\n",
    "pred1  = predict(modelrf, data = test)$predictions\n",
    "levels(test$y) = c(\"0\", \"1\")\n",
    "levels(pred1) = c(\"0\", \"1\")\n",
    "cm1=confusionMatrix(pred1, test$y, positive=\"1\")\n",
    "cm1\n",
    "\n",
    "  \n",
    "#glm\n",
    "  library(glmnet)\n",
    "modelglm <- train(\n",
    "  y~.,\n",
    "  data = train, method = \"glm\",\n",
    "  trControl = trainControl(method = \"cv\", number = 7, verboseIter = TRUE))\n",
    "pred2=predict(modelglm,test)\n",
    "levels(bank$y) = c(\"0\", \"1\")\n",
    "levels(pred2) = c(\"0\", \"1\")\n",
    "cm2=confusionMatrix(pred2, test$y, positive=\"1\")\n",
    "cm2\n",
    "\n",
    "#glmnet\n",
    "\n",
    "modelglmnet <- train(\n",
    "  y~.,\n",
    "  data = train, method = \"glmnet\",\n",
    "  trControl = trainControl(method = \"cv\", number = 7, verboseIter = TRUE))\n",
    "pred3=predict(modelglmnet,test)\n",
    "levels(test$y) = c(\"0\", \"1\")\n",
    "levels(pred3) = c(\"0\", \"1\")\n",
    "cm3=confusionMatrix(pred3, test$y, positive=\"1\")\n",
    "cm3\n",
    "\n",
    "#I also recoded categorical variables that had categories greater than 10. I started by categorizing months by seasons. I then categorized job into low class paying jobs, high class paying jobs, and unemployed. I also categorized education to before high school, high school degree, unviversity degree, and unknown. After recoding these variables, I ran the models again, and I found that it improved the glm and glmnet models by 1% in accuracy.\n",
    "\n",
    "#recoding categorical variables\n",
    "bank4=bank3\n",
    "library(car)\n",
    "Spring.set=c(\"mar\",\"apr\",\"may\")\n",
    "Summer.set=c(\"jun\",\"jul\",\"aug\")\n",
    "Autumn.set=c(\"sep\",\"oct\",\"nov\")\n",
    "Winter.set=c(\"dec\")\n",
    "bank4$month=recode(bank4$month,\"Spring.set='Spring'; Summer.set='Summer'; Autumn.set='Autumn'; Winter.set='Winter'\")\n",
    "levels(bank4$job)=c(\"low class\", \"low class\", \"high class\", \"low class\", \"high class\", \"unemployed\", \"low class\", \"low class\", \"low class\", \"low class\", \"unemployed\", \"unknown\")\n",
    "levels(bank4$education)=c(\"before high school\",\"before high school\", \"before high school\", \"high school\", \"before high school\", \"unknown\", \"college\", \"unknown\")\n",
    "\n",
    "#model\n",
    "set.seed(200)\n",
    "splitIndex<- createDataPartition(bank4$y, p=.7, list=FALSE, times = 1)\n",
    "train<- bank4[splitIndex,]\n",
    "test<- bank4[-splitIndex,]\n",
    "\n",
    "#decision tree model\n",
    "\n",
    "model <- train(y~.,data =train, method = \"rpart\")\n",
    "pred=predict(model,test)\n",
    "levels(test$y) = c(\"1\", \"0\")\n",
    "levels(pred) = c(\"1\", \"0\")\n",
    "cm=confusionMatrix(pred, test$y, positive=\"1\")\n",
    "cm\n",
    "\n",
    "  \n",
    "#random forest\n",
    "  \n",
    "modelrf = ranger(y ~., data = train)\n",
    "pred1  = predict(modelrf, data = test)$predictions\n",
    "levels(test$y) = c(\"0\", \"1\")\n",
    "levels(pred1) = c(\"0\", \"1\")\n",
    "cm1=confusionMatrix(pred1, test$y, positive=\"1\")\n",
    "cm1\n",
    "\n",
    "  \n",
    "#glm\n",
    "  library(glmnet)\n",
    "modelglm <- train(\n",
    "  y~.,\n",
    "  data = train, method = \"glm\",\n",
    "  trControl = trainControl(method = \"cv\", number = 7, verboseIter = TRUE))\n",
    "pred2=predict(modelglm,test)\n",
    "levels(test$y) = c(\"0\", \"1\")\n",
    "levels(pred2) = c(\"0\", \"1\")\n",
    "cm2=confusionMatrix(pred2, test$y, positive=\"1\")\n",
    "cm2\n",
    "\n",
    "#glmnet\n",
    "\n",
    "modelglmnet <- train(\n",
    "  y~.,\n",
    "  data = train, method = \"glmnet\",\n",
    "  trControl = trainControl(method = \"cv\", number = 7, verboseIter = TRUE))\n",
    "pred3=predict(modelglmnet,test)\n",
    "levels(bank$y) = c(\"0\", \"1\")\n",
    "levels(pred3) = c(\"0\", \"1\")\n",
    "cm3=confusionMatrix(pred3, test$y, positive=\"1\")\n",
    "cm3\n",
    "\n",
    "#I then tried encoding categorical variables by dummy coding. After using this method, my models did not improve.\n",
    "\n",
    "#encoding\n",
    "bank5=bank3\n",
    "preProcess_missingdata_model <- preProcess(bank, method='medianImpute')\n",
    "trainData <- predict(preProcess_missingdata_model, newdata =bank5)\n",
    "dummies_model <- dummyVars(y ~ ., data=train)\n",
    "trainData_mat <- predict(dummies_model, newdata = train)\n",
    "\n",
    "#model\n",
    "set.seed(200)\n",
    "splitIndex<- createDataPartition(bank5$y, p=.7, list=FALSE, times = 1)\n",
    "train<- bank5[splitIndex,]\n",
    "test<- bank5[-splitIndex,]\n",
    "\n",
    "#decision tree model\n",
    "\n",
    "model <- train(y~.,data =train, method = \"rpart\")\n",
    "pred=predict(model,test)\n",
    "levels(test$y) = c(\"1\", \"0\")\n",
    "levels(pred) = c(\"1\", \"0\")\n",
    "cm=confusionMatrix(pred, test$y, positive=\"1\")\n",
    "cm\n",
    "\n",
    "  \n",
    "#random forest\n",
    "  \n",
    "modelrf = ranger(y ~., data = train)\n",
    "pred1  = predict(modelrf, data = test)$predictions\n",
    "levels(test$y) = c(\"0\", \"1\")\n",
    "levels(pred1) = c(\"0\", \"1\")\n",
    "cm1=confusionMatrix(pred1, test$y, positive=\"1\")\n",
    "cm1\n",
    "\n",
    "  \n",
    "#glm\n",
    "  library(glmnet)\n",
    "modelglm <- train(\n",
    "  y~.,\n",
    "  data = train, method = \"glm\",\n",
    "  trControl = trainControl(method = \"cv\", number = 7, verboseIter = TRUE))\n",
    "pred2=predict(modelglm,test)\n",
    "levels(bank$y) = c(\"0\", \"1\")\n",
    "levels(pred2) = c(\"0\", \"1\")\n",
    "cm2=confusionMatrix(pred2, test$y, positive=\"1\")\n",
    "cm2\n",
    "\n",
    "#glmnet\n",
    "\n",
    "modelglmnet <- train(\n",
    "  y~.,\n",
    "  data = train, method = \"glmnet\",\n",
    "  trControl = trainControl(method = \"cv\", number = 7, verboseIter = TRUE))\n",
    "pred3=predict(modelglmnet,test)\n",
    "levels(test$y) = c(\"0\", \"1\")\n",
    "levels(pred3) = c(\"0\", \"1\")\n",
    "cm3=confusionMatrix(pred3, test$y, positive=\"1\")\n",
    "cm3\n",
    "\n",
    "#After running all the models, I found that the best model was glmnet after imputing numeric variables by mean and categorical variables by mode. I wanted to see if I could improve my best model by tuning, so I tuned the glmnet to see if it would give me a better result, however it did not improve the model. In conclusion, the best predicting model is the glmnet after imputing numeric variables by mean and categorical variables by mode. \n",
    "\n",
    "#glmnet tuned\n",
    "myGrid = expand.grid(alpha = 0:1,lambda = seq(0.0001,1,length = 10))\n",
    "\n",
    "modelglmnettun <- train(\n",
    "  y~.,\n",
    "  tuneLength = 1,\n",
    "  data = train, method = \"glmnet\",\n",
    "  trControl = trainControl(method = \"cv\", number = 4, verboseIter = TRUE),tuneGrid=myGrid)\n",
    "pred=predict(modelglmnettun,test)\n",
    "levels(test$y) = c(\"0\", \"1\")\n",
    "levels(pred) = c(\"0\", \"1\")\n",
    "cm=confusionMatrix(pred, test$y, positive=\"1\")\n",
    "cm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
